{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Model, load_model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# setup ROUGE scorer and tokenizer\n",
    "ROUGE_METRICS = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "scorer = rouge_scorer.RougeScorer(ROUGE_METRICS, use_stemmer=True)\n",
    "\n",
    "# Load trained encoder-decoder model\n",
    "encoder_model = load_model('../trained_models/encoder_model.h5', compile=False)\n",
    "decoder_model = load_model('../trained_models/decoder_model.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary:  45.7757871354066\n",
      "% of rare words in vocabulary:  48.27797149324035\n"
     ]
    }
   ],
   "source": [
    "post_pre = pd.read_csv('../data/ed_cleaned_data.csv')\n",
    "post_pre = post_pre.loc[:, ~post_pre.columns.str.contains('^Unnamed')]\n",
    "post_pre = post_pre.fillna(\"\")\n",
    "\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(\n",
    "    np.array(post_pre[\"text\"]),\n",
    "    np.array(post_pre[\"summary\"]),\n",
    "    test_size=0.1,\n",
    "    random_state=0,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "max_text_len = 100\n",
    "max_summary_len = 20\n",
    "\n",
    "art_tokenizer, x_tr, x_val = utils.build_tokenizer(x_tr, x_val, max_text_len)\n",
    "head_tokenizer, y_tr, y_val = utils.build_tokenizer(y_tr, y_val, max_summary_len)\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "art_voc = art_tokenizer.num_words + 1\n",
    "head_voc = head_tokenizer.num_words + 1\n",
    "\n",
    "reverse_target_word_index = head_tokenizer.index_word\n",
    "reverse_source_word_index = art_tokenizer.index_word\n",
    "target_word_index = head_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    (e_out, e_h, e_c) = encoder_model.predict(input_seq, verbose=False)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        (output_tokens, h, c) = decoder_model.predict([target_seq] + [e_out, e_h, e_c], verbose=False)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = random.choice(np.argsort(output_tokens[0, -1, :])[3:5])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'eostok':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find the stop word.\n",
    "        if sampled_token == 'eostok' or len(decoded_sentence.split()) >= max_summary_len - 1:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        (e_h, e_c) = (h, c)\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    # To convert sequence to summary\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0 and i != target_word_index['sostok'] and i != target_word_index['eostok']:\n",
    "            newString = newString + reverse_target_word_index[i] + ' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    # To convert sequence to text\n",
    "    newString = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0:\n",
    "            newString = newString + reverse_source_word_index[i] + ' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\n",
      "\tactual: start hillary offered nothing to nevertrump conservatives end \n",
      "\tpredic:  drought elie hollande e pay challenging assailant digs oppose ’gun confront carry corbyn profiling ouster recommend marco target’s beijing’s\n",
      "1:\n",
      "\tactual: start trump just made an unprecedented change to the national security council end \n",
      "\tpredic:  legalize ford reduce counting tortured syrian prior river gorsuch matter’ widow spox sabotage before before philippine frank christmas tiger\n",
      "2:\n",
      "\tactual: start risky flight from south pole arrives in chile end \n",
      "\tpredic:  legalize laden facebook fallen ’we fact fracking wake incidents lochte ballot y premier bieber manchin mcconnell veterans failures duke\n",
      "3:\n",
      "\tactual: start trump tells reporters work just ask israel’ end \n",
      "\tpredic:  legalize adviser face a politics’ nationalists ’trump partners prominent viola create spent organization pipeline runner courage building manuel ticket\n",
      "4:\n",
      "\tactual: start exclusive—regnery why law enforcement will like justice neil gorsuch end \n",
      "\tpredic:  drought practices sony o’donnell meat 500 vatican aside neighbors mayer foreigners clearance white insider win’ values measures country’ gears\n",
      "5:\n",
      "\tactual: start harry belafonte don’t die’ end \n",
      "\tpredic:  drought elie eating afd live criminals money video beck hackers cia hurt mayhem cup shotgun meets typhoon philippines luther\n",
      "6:\n",
      "\tactual: start breitbart news daily trump addresses congress end \n",
      "\tpredic:  drought sony firestorm pentagon bruce video video hollywood span holmes john boy anti mcmullin fearful kevin retaliation range size\n",
      "7:\n",
      "\tactual: start migrants settled in hotel hosting furry convention end \n",
      "\tpredic:  drought practices sony sweeps cost rages session considered identify towards carrying dilma dress suits pepsi norway republic size sex\n",
      "8:\n",
      "\tactual: start how emotion over pet care helps explain human health spending end \n",
      "\tpredic:  drought sony harley factory spain irvine uneasy crashed parenthood ray boehner newsom overnight defeated good gains tide ii t’\n",
      "9:\n",
      "\tactual: start evan mcmullin president donald trump is enemy end \n",
      "\tpredic:  drought practices marathon notes plant millennials carrying krauthammer auction daughters bears consider immigrant weigh atheist construction pedestrians cory physics\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alecc\\Documents\\Natural Language Processing\\NLPFinalProject\\src\\ed_eval.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alecc/Documents/Natural%20Language%20Processing/NLPFinalProject/src/ed_eval.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1000\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alecc/Documents/Natural%20Language%20Processing/NLPFinalProject/src/ed_eval.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     actual_headline \u001b[39m=\u001b[39m seq2summary(y_tr[i])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alecc/Documents/Natural%20Language%20Processing/NLPFinalProject/src/ed_eval.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     predicted_headline \u001b[39m=\u001b[39m decode_sequence(x_tr[i]\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, max_text_len))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alecc/Documents/Natural%20Language%20Processing/NLPFinalProject/src/ed_eval.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     ah[i] \u001b[39m=\u001b[39m actual_headline\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alecc/Documents/Natural%20Language%20Processing/NLPFinalProject/src/ed_eval.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     ph[i] \u001b[39m=\u001b[39m predicted_headline\n",
      "\u001b[1;32mc:\\Users\\alecc\\Documents\\Natural Language Processing\\NLPFinalProject\\src\\ed_eval.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alecc/Documents/Natural%20Language%20Processing/NLPFinalProject/src/ed_eval.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m decoded_sentence \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alecc/Documents/Natural%20Language%20Processing/NLPFinalProject/src/ed_eval.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m stop_condition:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alecc/Documents/Natural%20Language%20Processing/NLPFinalProject/src/ed_eval.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     (output_tokens, h, c) \u001b[39m=\u001b[39m decoder_model\u001b[39m.\u001b[39;49mpredict([target_seq] \u001b[39m+\u001b[39;49m [e_out, e_h, e_c], verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alecc/Documents/Natural%20Language%20Processing/NLPFinalProject/src/ed_eval.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# Sample a token\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alecc/Documents/Natural%20Language%20Processing/NLPFinalProject/src/ed_eval.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     sampled_token_index \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(np\u001b[39m.\u001b[39margsort(output_tokens[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])[\u001b[39m3\u001b[39m:\u001b[39m5\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\alecc\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\alecc\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:2220\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2211\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2212\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2213\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2214\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2217\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2218\u001b[0m         )\n\u001b[1;32m-> 2220\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   2221\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   2222\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2223\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   2224\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2225\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2226\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2227\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2228\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2229\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2230\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[0;32m   2231\u001b[0m )\n\u001b[0;32m   2233\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\alecc\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1582\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1581\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1582\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alecc\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:1262\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1259\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1261\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1263\u001b[0m     x,\n\u001b[0;32m   1264\u001b[0m     y,\n\u001b[0;32m   1265\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1266\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1267\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1268\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1269\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1270\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1271\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1272\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1273\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1274\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1275\u001b[0m )\n\u001b[0;32m   1277\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alecc\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\data_adapter.py:308\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m indices\n\u001b[0;32m    303\u001b[0m \u001b[39m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[39m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[39m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m# performance.\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mmap(permutation)\u001b[39m.\u001b[39;49mprefetch(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mslice_batch_indices\u001b[39m(indices):\n\u001b[0;32m    311\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \n\u001b[0;32m    313\u001b[0m \u001b[39m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39m      A Dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alecc\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1326\u001b[0m, in \u001b[0;36mDatasetV2.prefetch\u001b[1;34m(self, buffer_size, name)\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[39mif\u001b[39;00m DEBUG_MODE:\n\u001b[0;32m   1325\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[1;32m-> 1326\u001b[0m \u001b[39mreturn\u001b[39;00m PrefetchDataset(\u001b[39mself\u001b[39;49m, buffer_size, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\alecc\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5675\u001b[0m, in \u001b[0;36mPrefetchDataset.__init__\u001b[1;34m(self, input_dataset, buffer_size, slack_period, name)\u001b[0m\n\u001b[0;32m   5671\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   5672\u001b[0m \u001b[39m# We colocate the prefetch dataset with its input as this collocation only\u001b[39;00m\n\u001b[0;32m   5673\u001b[0m \u001b[39m# happens automatically in graph mode.\u001b[39;00m\n\u001b[0;32m   5674\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mcolocate_with(input_dataset\u001b[39m.\u001b[39m_variant_tensor):\n\u001b[1;32m-> 5675\u001b[0m   variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mprefetch_dataset(\n\u001b[0;32m   5676\u001b[0m       input_dataset\u001b[39m.\u001b[39m_variant_tensor,\n\u001b[0;32m   5677\u001b[0m       buffer_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_size,\n\u001b[0;32m   5678\u001b[0m       slack_period\u001b[39m=\u001b[39mslack_period,\n\u001b[0;32m   5679\u001b[0m       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n\u001b[0;32m   5680\u001b[0m \u001b[39msuper\u001b[39m(PrefetchDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[1;32mc:\\Users\\alecc\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:6159\u001b[0m, in \u001b[0;36mprefetch_dataset\u001b[1;34m(input_dataset, buffer_size, output_types, output_shapes, slack_period, legacy_autotune, buffer_size_min, metadata, name)\u001b[0m\n\u001b[0;32m   6157\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   6158\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 6159\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   6160\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mPrefetchDataset\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_dataset, buffer_size,\n\u001b[0;32m   6161\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types, \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes,\n\u001b[0;32m   6162\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mslack_period\u001b[39;49m\u001b[39m\"\u001b[39;49m, slack_period, \u001b[39m\"\u001b[39;49m\u001b[39mlegacy_autotune\u001b[39;49m\u001b[39m\"\u001b[39;49m, legacy_autotune,\n\u001b[0;32m   6163\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mbuffer_size_min\u001b[39;49m\u001b[39m\"\u001b[39;49m, buffer_size_min, \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m, metadata)\n\u001b[0;32m   6164\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   6165\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ah = {} # GET TESTING HEADLINES FOR EACH DATASET\n",
    "ph = {} # GET ED PREDICTED HEADLINES FOR EACH DATASET\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    actual_headline = seq2summary(y_tr[i])\n",
    "    predicted_headline = decode_sequence(x_tr[i].reshape(1, max_text_len))\n",
    "    ah[i] = actual_headline\n",
    "    ph[i] = predicted_headline\n",
    "    print(f'{i}:')\n",
    "    print(f'\\tactual: {actual_headline}')\n",
    "    print(f'\\tpredic: {predicted_headline}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
