{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AdamWeightDecay, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import utils\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "Num GPUs Available:  0\n",
      "GPUs: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "# Constants\n",
    "TLDR = ' TL;DR '\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/cleaned_data/\"\n",
    "if not os.path.exists(datapath):\n",
    "    utils.clean_data()\n",
    "all_articles_dict = utils.load_article_data(path=datapath)\n",
    "del all_articles_dict['clean_Articles.csv']\n",
    "del all_articles_dict['clean_CNN_Articels_clean.csv']\n",
    "all_articles_df = pd.concat([df for df in all_articles_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data by: article TL;DR headline\n",
    "num_elements = 1000\n",
    "all_articles = all_articles_df.values.tolist()\n",
    "all_articles = [x[1].strip() + \" TL;DR \" + x[0].strip().replace(' - The New York Times', '') \n",
    "                for x in all_articles \n",
    "                if isinstance(x[0], str) and isinstance(x[1], str)][0:num_elements]\n",
    "\n",
    "def pad_and_truncate_data(dataset):\n",
    "    \"\"\"\n",
    "    Format data to always contain the TL;DR and the entire headline. Truncate the article such that\n",
    "    the whole string becomes MAX_LEN long.\n",
    "    \"\"\"\n",
    "    ARTICLE_LEN = MAX_LEN - len(TLDR)\n",
    "    result = []\n",
    "    for d in dataset:\n",
    "        article, headline = d.split(' TL;DR ')\n",
    "        result.append(article[0:ARTICLE_LEN - len(headline)] + TLDR + headline)\n",
    "    return result\n",
    "\n",
    "all_articles = pad_and_truncate_data(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to files to be loaded into a dataset\n",
    "random.seed(11)\n",
    "random.shuffle(all_articles)\n",
    "TRAIN_SPLIT = 0.9\n",
    "END_IDX = int(len(all_articles) * TRAIN_SPLIT)\n",
    "with open(\"../data/train_data.txt\", \"w\", encoding='utf-8') as txt_file:\n",
    "    for line in all_articles[0:END_IDX]:\n",
    "        txt_file.write(line + \"\\n\") # works with any number of elements in a line\n",
    "with open(\"../data/test_data.txt\", \"w\", encoding='utf-8') as txt_file:\n",
    "    for line in all_articles[END_IDX:]:\n",
    "        txt_file.write(line + \"\\n\") # works with any number of elements in a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 62.27it/s]\n",
      "Generating train split: 900 examples [00:00, 41874.18 examples/s]\n",
      "Generating validation split: 100 examples [00:00, 5160.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"text\", data_files={\"train\": '../data/train_data.txt', \"validation\": '../data/test_data.txt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'The presidency of Donald J. Trump has been noteworthy for its speed. In his first week in office, as the president’s aides won’t tire of reminding us, Mr. Trump has already put in motion plans to do much of what he promised to do while campaigning. But it’s not just the politician who is moving fast. It’s the population, too. In a matter of hours on Saturday, thousands rushed to the nation’s airports, beckoned by tweets. The f TL;DR The Alt-Majority: How Social Networks Empowered Mass Protests Against Trump'}\n",
      "900\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(datasets[\"train\"][10])\n",
    "print(len(datasets['train']))\n",
    "print(len(datasets['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        return self.tokenizer(examples[\"text\"],\n",
    "                              padding='max_length',\n",
    "                              truncation=True,\n",
    "                              max_length=self.max_len // 4)\n",
    "\n",
    "tokenizer_wrapper = TokenizerWrapper(tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 900/900 [00:06<00:00, 132.41 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 100/100 [00:04<00:00, 21.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize data\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenizer_wrapper.tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5673, 447, 247, 21358, 5984, 5883, 3955, 11, 2688, 5018, 220, 851, 220, 220, 383, 717, 11903, 286, 6669, 447, 247, 1000, 1215, 388, 320, 11, 257, 289, 6548, 1748, 319, 262, 10183, 30140, 286, 10843, 11, 389, 783, 220, 764, 317, 27316, 3443, 4721, 938, 614, 11, 290, 2319, 5085, 389, 11694, 612, 11, 749, 2636, 286, 3288, 5640, 706, 890, 290, 12309, 3160, 13, 1320, 318, 284, 910, 11, 612, 318, 2147, 8584, 546, 428, 1295, 11, 530, 286, 262, 11706, 18573, 284, 10843, 287, 262, 12030, 2688, 5018, 11, 543, 2692, 12000, 422, 8078, 2026, 812, 2084, 13, 564, 250, 1026, 447, 247, 82, 636, 220, 24811, 26, 7707, 2692, 447, 247, 82, 6912, 12, 14993, 364, 16168, 284, 564, 246, 5247, 4403, 447, 247], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "MA’ALE ADUMIM, West Bank  —   The first babies of Ma’ale Adumim, a hilly city on the eastern outskirts of Jerusalem, are now . A cemetery finally opened last year, and 40 residents are buried there, most dead of natural causes after long and peaceful lives. That is to say, there is nothing temporary about this place, one of the closest settlements to Jerusalem in the occupied West Bank, which Israel seized from Jordan 50 years ago. “It’s part  TL;DR Israel’s Hard-Liners Want to ‘Go Big’\n",
      "491\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets[\"train\"][1])\n",
    "print(tokenizer.decode(tokenized_datasets[\"train\"][1][\"input_ids\"]))\n",
    "print(len(tokenizer.decode(tokenized_datasets[\"train\"][1][\"input_ids\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 900/900 [00:02<00:00, 353.64 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 100/100 [00:02<00:00, 43.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Add labels to tokenized data\n",
    "def add_labels(examples):\n",
    "    examples['labels'] = examples['input_ids'].copy()\n",
    "    return examples\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    add_labels,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and validation datasets\n",
    "train_set = model.prepare_tf_dataset(\n",
    "    lm_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "validation_set = model.prepare_tf_dataset(\n",
    "    lm_datasets[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alecc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\optimizers\\legacy\\adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alecc\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "225/225 [==============================] - 534s 2s/step - loss: 3.2039 - val_loss: 2.7369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d432702ad0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile and train model\n",
    "optimizer = AdamWeightDecay(lr=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)\n",
    "model.fit(train_set, \n",
    "          validation_data=validation_set, \n",
    "          epochs=1,  \n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('../trained_models/gpt2-summarization')\n",
    "model.save_pretrained('../trained_models/gpt2-summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../trained_models/gpt2-summarization/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForCausalLM.from_pretrained('../trained_models/gpt2-summarization/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
