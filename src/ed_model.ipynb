{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, RepeatVector, concatenate, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_ARTICLES_FILE = '../data/cleaned_articles_ed.txt'\n",
    "CLEANED_HEADLINES_FILE = '../data/cleaned_headlines_ed.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLEANED_ARTICLES_FILE,'r', encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    articles = data.split('\\n')     \n",
    "    \n",
    "with open(CLEANED_HEADLINES_FILE,'r', encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    headlines = data.split('\\n') \n",
    "    \n",
    "articles = articles[:200]\n",
    "headlines = headlines[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_tokenizer, encoded_art = utils.create_tokenizer(articles)\n",
    "head_tokenizer, encoded_head = utils.create_tokenizer(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2564\n",
      "724\n",
      "200\n",
      "200\n",
      "[ 216    1  144   73   16  286    2  446  158  607  995  287   11  996\n",
      "   36  186  159    2  608  288    4   53  447   18   11    1   58   73\n",
      "  124  109  385   89   19  288    4  287   22   39  609    7  607  995\n",
      " 1000    9  160 1001    3    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[ 43  30 231   3  44 232 134 135  14 233 234 235   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 55\n",
    "MAX_HEADLINE_LENGTH = 15\n",
    "\n",
    "encoder_vocab_size = len(art_tokenizer.word_index) + 1\n",
    "decoder_vocab_size = len(head_tokenizer.word_index) + 1\n",
    "\n",
    "print(encoder_vocab_size)\n",
    "print(decoder_vocab_size)\n",
    "\n",
    "\n",
    "src_txt_length = len(encoded_art)\n",
    "sum_txt_length = len(encoded_head)\n",
    "\n",
    "print(src_txt_length)\n",
    "print(sum_txt_length)\n",
    "\n",
    "padded_encoded_art = pad_sequences(encoded_art,  maxlen=src_txt_length, padding='post')\n",
    "padded_encoded_head = pad_sequences(encoded_head,  maxlen=sum_txt_length, padding='post')\n",
    "\n",
    "print((padded_encoded_art[0]))\n",
    "print((padded_encoded_head[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(padded_encoded_art), np.array(padded_encoded_head), test_size=.10, random_state=82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]                0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 200, 128)             328192    ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, None, 128)            92672     ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 [(None, 200, 128),           131584    ['embedding[0][0]']           \n",
      "                              (None, 128),                                                        \n",
      "                              (None, 128)]                                                        \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, None, 128)            131584    ['embedding_1[0][0]',         \n",
      "                                                                     'lstm[0][1]',                \n",
      "                                                                     'lstm[0][2]']                \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 724)            93396     ['lstm_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 777428 (2.97 MB)\n",
      "Trainable params: 777428 (2.97 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/\n",
    "\n",
    "#I would choose Alternate 1 from this link\n",
    "#That I belive is how we should create our encoder deccoder model\n",
    "\n",
    "#NOTE: Example code here\n",
    "# article input model\n",
    "# inputs1 = Input(shape=(src_txt_length,))\n",
    "# article1 = Embedding(encoder_vocab_size, 128)(inputs1)\n",
    "# article2 = LSTM(128)(article1)\n",
    "# article3 = RepeatVector(sum_txt_length)(article2)\n",
    "# # summary input model\n",
    "# inputs2 = Input(shape=(sum_txt_length,))\n",
    "# summ1 = Embedding(encoder_vocab_size, 128)(inputs2)\n",
    "\n",
    "\n",
    "# # decoder model\n",
    "# decoder1 = concatenate([article3, summ1])\n",
    "# decoder2 = LSTM(128)(decoder1)\n",
    "# outputs = Dense(decoder_vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# # tie it together [article, summary] [word]\n",
    "# model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# encoder input model\n",
    "encoder_inputs = Input(shape=(src_txt_length,))\n",
    "encoder_emb = Embedding(encoder_vocab_size, 128, trainable=True)(encoder_inputs)\n",
    "encoder_lstm1 = LSTM(128, return_sequences=True, return_state=True, dropout=.4, recurrent_dropout=.4)\n",
    "\n",
    "encoder_output1, state_h, state_c = encoder_lstm1(encoder_emb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# decoder output model\n",
    "decoder_inputs = Input(shape=(None, ))\n",
    "decoder_emb_layer = Embedding(decoder_vocab_size, 128)\n",
    "decoder_emb = decoder_emb_layer(decoder_inputs)\n",
    "decoder_lstm1 = LSTM(128, return_sequences=True, dropout=.4, recurrent_dropout=.4)\n",
    "\n",
    "decoder_outputs = decoder_lstm1(decoder_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# dense layer\n",
    "decoder_outputs = (Dense(decoder_vocab_size, activation='softmax'))(decoder_outputs)\n",
    "\n",
    "# tie it together\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# es = EarlyStopping(monitor='val-loss', mode='min', verbose=1, patience=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://medium.com/@duncanboynton/seq2seq-news-article-summary-using-an-encoder-decoder-lstm-to-summarize-text-5de56fccfbf6\n",
    "\n",
    "# # 10 points\n",
    "\n",
    "# def data_generator(X: list, y: list, num_sequences_per_batch: int, vocab_size: int) -> (np.array,np.array):\n",
    "#     '''\n",
    "#     Returns data generator to be used by feed_forward\n",
    "#     https://wiki.python.org/moin/Generators\n",
    "#     https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "#     Yields batches of embeddings and labels to go with them.\n",
    "#     Use one hot vectors to encode the labels \n",
    "#     (see the to_categorical function)\n",
    "    \n",
    "#     If for_feedforward is True: \n",
    "#     Returns data generator to be used by feed_forward\n",
    "#     else: Returns data generator for RNN model\n",
    "#     '''    \n",
    "#     one = []\n",
    "#     two = []    \n",
    "#     for idx, i in enumerate(X):\n",
    "#         if idx > 0 and idx % (num_sequences_per_batch) == 0:\n",
    "#             yield np.array(one), to_categorical(two, num_classes=vocab_size)\n",
    "#             one = []\n",
    "#             two = []\n",
    "#         one.append(i)\n",
    "#         two.append(y[idx])\n",
    "        \n",
    "        \n",
    "# word_generator = data_generator(X_train, y_train, 100, decoder_vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2/2 [==============================] - 17s 2s/step - loss: 6.5340\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 5s 2s/step - loss: 6.0590\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 5s 2s/step - loss: 4.0008\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 5s 2s/step - loss: 2.2602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1757a059210>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# steps_per_epoch_words = len(X_train)//128  # Number of batches per epoch\n",
    "\n",
    "model.fit(\n",
    "    [X_train, y_train[:,:-1]],\n",
    "    y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:,1:],\n",
    "    epochs=4,\n",
    "    batch_size=100)\n",
    "    # validation_data=([X_test, y_test[:, :-1]], y_test.reshape(y_test.shape[0], y_test.shape[1], 1)[:, 1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
