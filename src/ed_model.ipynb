{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, RepeatVector, concatenate, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_ARTICLES_FILE = '../data/cleaned_articles_ed.txt'\n",
    "CLEANED_HEADLINES_FILE = '../data/cleaned_headlines_ed.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLEANED_ARTICLES_FILE,'r', encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    articles = data.split('\\n')     \n",
    "    \n",
    "with open(CLEANED_HEADLINES_FILE,'r', encoding=\"utf-8\") as file:\n",
    "    data = file.read()\n",
    "    headlines = data.split('\\n') \n",
    "    \n",
    "articles = articles[:1000]\n",
    "headlines = headlines[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_tokenizer, encoded_art = utils.create_tokenizer(articles)\n",
    "head_tokenizer, encoded_head = utils.create_tokenizer(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7669\n",
      "2307\n",
      "1000\n",
      "1000\n",
      "[1401  485    1  229   45   17  284    3  880  115  421 3469  713   12\n",
      " 1435   46  449  265    3  787  371    4  102  881   21   12    1   52\n",
      "   45  335  196  323  182   13  371    4  713   25   34 1436    8  421\n",
      " 3469 3473    9  197 2320    2 1401    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0]\n",
      "[ 84  71 217   4  67 974 602 438  20 975 976 977   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 1000\n",
    "MAX_HEADLINE_LENGTH = 1000\n",
    "\n",
    "encoder_vocab_size = len(art_tokenizer.word_index)\n",
    "decoder_vocab_size = len(head_tokenizer.word_index)\n",
    "\n",
    "print(encoder_vocab_size)\n",
    "print(decoder_vocab_size)\n",
    "\n",
    "\n",
    "src_txt_length = len(encoded_art)\n",
    "sum_txt_length = len(encoded_head)\n",
    "\n",
    "print(src_txt_length)\n",
    "print(sum_txt_length)\n",
    "\n",
    "padded_encoded_art = pad_sequences(encoded_art,  maxlen=MAX_LENGTH, padding='post')\n",
    "padded_encoded_head = pad_sequences(encoded_head,  maxlen=MAX_HEADLINE_LENGTH, padding='post')\n",
    "\n",
    "print((padded_encoded_art[0]))\n",
    "print((padded_encoded_head[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(padded_encoded_art), np.array(padded_encoded_head), test_size=.10, random_state=82)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 1000)]            0         \n",
      "                                                                 \n",
      " embedding_17 (Embedding)    (None, 1000, 128)         981632    \n",
      "                                                                 \n",
      " lstm_28 (LSTM)              (None, 128)               131584    \n",
      "                                                                 \n",
      " repeat_vector_14 (RepeatVe  (None, 1000, 128)         0         \n",
      " ctor)                                                           \n",
      "                                                                 \n",
      " lstm_29 (LSTM)              (None, 1000, 128)         131584    \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1000, 2307)        297603    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1542403 (5.88 MB)\n",
      "Trainable params: 1542403 (5.88 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/\n",
    "\n",
    "#I would choose Alternate 1 from this link\n",
    "#That I belive is how we should create our encoder deccoder model\n",
    "\n",
    "#NOTE: Example code here\n",
    "# article input model\n",
    "# inputs1 = Input(shape=(src_txt_length,))\n",
    "# article1 = Embedding(encoder_vocab_size, 128)(inputs1)\n",
    "# article2 = LSTM(128)(article1)\n",
    "# article3 = RepeatVector(sum_txt_length)(article2)\n",
    "# # summary input model\n",
    "# inputs2 = Input(shape=(sum_txt_length,))\n",
    "# summ1 = Embedding(encoder_vocab_size, 128)(inputs2)\n",
    "\n",
    "\n",
    "# # decoder model\n",
    "# decoder1 = concatenate([article3, summ1])\n",
    "# decoder2 = LSTM(128)(decoder1)\n",
    "# outputs = Dense(decoder_vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# # tie it together [article, summary] [word]\n",
    "# model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# encoder input model\n",
    "inputs = Input(shape=(src_txt_length,))\n",
    "encoder1 = Embedding(encoder_vocab_size, 128)(inputs)\n",
    "encoder2 = LSTM(128)(encoder1)\n",
    "encoder3 = RepeatVector(sum_txt_length)(encoder2)\n",
    "\n",
    "# decoder output model\n",
    "decoder1 = LSTM(128, return_sequences=True)(encoder3)\n",
    "outputs = Dense(decoder_vocab_size, activation='softmax')(decoder1)\n",
    "\n",
    "# tie it together\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@duncanboynton/seq2seq-news-article-summary-using-an-encoder-decoder-lstm-to-summarize-text-5de56fccfbf6\n",
    "\n",
    "# 10 points\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, vocab_size: int) -> (np.array,np.array):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    If for_feedforward is True: \n",
    "    Returns data generator to be used by feed_forward\n",
    "    else: Returns data generator for RNN model\n",
    "    '''    \n",
    "    one = []\n",
    "    two = []    \n",
    "    for idx, i in enumerate(X):\n",
    "        if idx > 0 and idx % (num_sequences_per_batch) == 0:\n",
    "            yield np.array(one), to_categorical(two, num_classes=vocab_size)\n",
    "            one = []\n",
    "            two = []\n",
    "        one.append(i)\n",
    "        two.append(y[idx])\n",
    "        \n",
    "        \n",
    "word_generator = data_generator(X_train, y_train, 100, decoder_vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 18s 18s/step - loss: 7.7463\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 7.7195\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 15s 15s/step - loss: 7.6877\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 7.6387\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 14s 14s/step - loss: 7.5537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x25a6249d0f0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch_words = len(X_train)//100  # Number of batches per epoch\n",
    "\n",
    "\n",
    "model.fit(word_generator, epochs=5, steps_per_epoch=steps_per_epoch_words//5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 1s/step\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['following nestle map rig signed q tumbles forb jet cr ', 'fuel praises resigns 6.19 resist contract nasdaq covering shipbuilder collapse ', 'prolonged files observe invite 146 moratorium gunmaker acknowledges with completed ', 'final selfie per billi sou nawaz votes operations jan income ', 'itc thirsty occurs less chance forge rs5 ongoing domestic files ', 'rs739 shipbuilder 2nd salaried ventures quaid sale nissan washington 742 ', 'mixed feet back acknowledges 20521 curbs russia limited agency spain ', 'sou cheered tig based against review shaky pcin private should ', 'books finalised paradise covering highs september soft 610 exporters 9 ', 'expat operati chinese itc resources items ron add payment slump ', '18.20 modi most fy16 supply per helping futuretech payers row ', 'engines shops remaining guessed wrong 3565 worri chief govts sustain ', 'yemen seeking dubai ups downgrades needs stronger cess currenci gulf ', 'ireland norways basically revises months overcome aircraft edged talk eco ', 'rs18800 online pays shoppers linkedins 77 fares inflows bahamasl beauty ', 'consecutive shortage reference fees arab association ri karachiites a 108 ', 'finish 2 indian 75pc ob stand uncertainty add halal fears ', '1.2 potential charg dollars gain benefits energy hig 81 welcomes ', 'coup 0.90 khokhar holds tajikistan thinned gradually expectati optimi markets ', 'mln dairy electricity lifted turkey petering up akd old raja ', 'overseas recessi line discovery movement 411 manufacturing fixed deduction approved ', 'an oi million down reasonable m sargodha contract hr thursday ', 'disclosure lucky giants step agency powerful leads 518 resigns volatility ', 'sind 587476 exchange interested boosts 6.19 dovish incentives lifted worse ', 'ny <s> 42 insurance plummeting responsible diverg extend cou crude ', 'unveiled nerves stipend forb launch boeings mineral 109 kg bmi ', 'lng businessmen steep powers urges october itc uni concer questi ', 'grid resources trades review 1.6 spending petrol russia dollar pursue ', 'ogra intention breather ntn stockpile 518 lira silver soars tender ', 'duty bet shortfall 97 raisin 1.3 economy sli commercial onion ', 'japan moot govts hosts officials lower abid commodities fbr kasa ', 'welspun chi led wanted reopen cooperation inspection worst reveals regains ', 'cabinet </s> countries perfor permanent defend safe 1000 underpinned carrying ', 'revises japan federal 23bln firm bigger salaried holds build contr ', 'set jobs appreciates sunflower scheme possible place turn mineral caug ', 'running speculators myanmars timelines manufacturing utility find lynched trades dives ', 'maxus fear bids abid portugal crisis appreciates foreign ntn hong ', 'passes stays talc privatize nandipur push hosts procedure deadline opening ', 'subdued public throws diverg wal balochi directions resoluti mscu venture ', 'card soft connected nears client walk meets mineral strategy highlights ', 'import focu dismisses tighten create count zarb lows pipeli predi ', 'benefits dar air pm brexit demand rs5litre criticizes jet signed ', 'cargo building taxpayers commi storage benefit onion black other bln ', 'list 0.90 send threatens 81 pia tackling dual group maintain ', 'harvest yuan fund march rupee inspection pak kicks five insurance ', 'bahria revenue insurance step strugg showed uk stipend 3.4 dev ', 'kashmir corporation fed president rcci proj policies lev esti illegal ', 'smartphones tensi skids dovish as 1.45tr worri par br car ', 'iranian announces isr add far mor moscows ppa achieved oil ', 'beneficiaries plans crater focu balochi rs30 west linger refreshed shed ', 'fruits shed forecasts wears eyes comic termi rig flexibility economi ', 'presented equity cnic post basket portugal visa gupta cross crown ', '4.313 diversification diamonds revises cotton negotiati resigns dashes tapi firm ', 'businesses holds more years talk sale stays unchanged moves numbers ', 'acti carrying competitiveness export plunges consumers looks 309 plung raises ', 'wb germany khurram modis 10900 cancelled people recent according saudi ', 'dow across 22 25 get little emissions tech corruption soon ', 'caused struggle procedure vehicles outag chuxing immovable 15102 agr big ', 'decides assistant middle higher releases agriculture record defends desert better ', 'medals poi dips duti robust shipbuilder close paradise easing misses ', 'months charg advertising lahore bahrains counci uplift ir 0.90 dovish ', 'slightly lucky projects completed barrier showed maxus barr trades ishaq ', 'cancelled strategy al speaker 178 sher 7pc 1500 iea adv ', 'worsening 108 6s furnace samsung pursue finalises underpinned funding timelines ', 'turkey final acknowledges senior fy16 hitting awarded rs500 forecast raising ', 'subs bahamasl 610 more dashes australia stays sbr balochi last ', 'weig crude travel agrees backs legal store strength terror commitment ', 'graft safety govt final premature rescues outpu cosmetics worries assistance ', 'windows a officials resolution mixed unstab clean 1 stable before ', 'out allowed indus powder into profitab plummeting meter sea months ', 'weakens barrels adv muted failings cash consecutive benefits powder acknowledges ', 'around adb changer files advisers modified reference beneficiaries welcomes pact ', 'chief 2 inspection one strong muslims admits canadian engines div ', '2mln wealth interest summer offering inv balance weig respite payment ', 'half drought brimming rs5litre infrastructure index shape sli uae fy15 ', 'crater goodbye dovish cross 1.3 kasb higher 146 bln ti ', 'inflation50 slips suspends sharp collection 2157 barr unveiled karachi optimi ', 'users guessed new cigarette hsbc sell serve piles resources strike ', 'appreciates retur erase shortage lease year profit pi revenue public ', 'budg profit optimi ogra standard iranian president short conclude billions ', 'cold reach integral taxes issue golds bn transaction up drought ', '104 protectioni 40 export luxury vw off legislative drop 2016early ', 'clean brings companies appreciates indian dealerships raised 178 92 sign ', 'flat feel tensions shortages cameron 18 record sinks continues begins ', 'temporary shops hailing weeks three shahbaz caused ex targets billions ', 'boeings safe slid general nerves arrives launched unveil confidence shipment ', 'basically middle fpcci government defend paper 10th b counci complete ', 'drops egyptian engineering apparel isr tensions creams brexit plot regains ', 'run 1bn percent kg chipotle bezos discount chi approves general ', 'mobilink sacrifi raisin tobacco defaulters running billions ventures committee above ', 'fed cases import ny dismisses coordinate chinese 5.5 induct plummeting ', 'mn greek 330 chi mn trains asif 626 sees bn ', 'hike fi week yor drought falter era rou no cargoes ', 'ob consolidati mobile bids raisin launched night more reviv awards ', 'risk flower investors takes missed failed showcase illicit bou auction ', 'merge asia giants plunge arabia nabs count comments launches reveals ', 'observed invests try agriculture mobiles keeps bisp fitr fares reduced ', 'night e msci governor producer defend barr apparel long 31.921 ', 'bond association jumps easy meets stockpile fair allocation tpl auto ', 'missiles speaker assembly taking gawadar uni inflows cli responsible lease ']\n",
      "s islamabad directorate general of immigration and passports has contributed rs 86 billion to national exchequer on account of passport and visa fee during last five years the introduction of new measures as per modern requirements by ministry of interior have resulted in enhanced earning in passport and visa fee s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final = []\n",
    "temp = ''\n",
    "for i in result:\n",
    "    for j in range(10):\n",
    "        weights = i[j]\n",
    "        idx = random.choices(range(decoder_vocab_size), weights=weights)[0]\n",
    "        temp = temp + head_tokenizer.index_word[idx] + ' '\n",
    "    final.append(temp)\n",
    "    temp = ''\n",
    "    \n",
    "print(final)\n",
    "\n",
    "print(' '.join([art_tokenizer.index_word[i] for i in X_test[0] if i != 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
